# -*- coding: utf-8 -*-
"""data_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xabvsk3Y19a8qRu_6KJWft_SBGpZcx79

# 1. Install required libraries
"""

pip install python-dotenv

import requests
import json
import time
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
import os

"""# 2. Initial config"""

BASE_URL = "https://api.github.com"

# For this step is recommended to have a Github Personal access token (PAT)
# Go to GitHub Settings > Developer settings > Personal access tokens
TOKEN = os.getenv("GITHUB_TOKEN")  # Replace with your (PAT)

HEADERS = {
    "Authorization": f"token {TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

response = requests.get("https://api.github.com/user", headers=HEADERS)
print(response.status_code, response.text)

"""#3.1. Rate limit"""

def handle_rate_limit(response):
    """Handle GitHub API rate limiting by waiting if needed."""
    if response.status_code in (403, 429):
        reset_time = int(response.headers.get('X-RateLimit-Reset', 0))
        current_time = int(time.time())
        wait_time = reset_time - current_time + 1
        if wait_time > 0:
            print(f"Rate limit exceeded. Waiting {wait_time} seconds...")
            time.sleep(wait_time)
    return response

"""#3.2. API Request"""

def make_api_request(url, params=None):
    """Make a GitHub API GET request with error and rate-limit handling."""
    try:
        response = requests.get(url, headers=HEADERS, params=params)
        response = handle_rate_limit(response)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"Error {response.status_code}: {response.text}")
            return None
    except Exception as e:
        print(f"Request failed: {str(e)}")
        return None

"""#4.1. Search repositories"""

def search_repositories(query, sort="stars", order="desc", per_page=30):
    """Search GitHub repositories using the API."""
    url = f"{BASE_URL}/search/repositories"
    params = {
        "q": query,
        "sort": sort,
        "order": order,
        "per_page": per_page
    }
    data = make_api_request(url, params)
    if data:
        return data.get("items", [])
    return []

"""# 4.2. Get repo function"""

def get_repository_commits(owner, repo, per_page=30):
    """Get commits from a repository"""
    url = f"{BASE_URL}/repos/{owner}/{repo}/commits"
    params = {"per_page": per_page}

    return make_api_request(url, params)

"""#4.3. Get contents function"""

def get_repository_contents(owner, repo, path=""):
    """Get repository contents"""
    url = f"{BASE_URL}/repos/{owner}/{repo}/contents/{path}"

    return make_api_request(url)

"""#4.1. Paginate repositories"""

def paginate_repositories(query, max_pages=3, per_page=30):
    all_repos = []
    for page in range(1, max_pages + 1):
        params = {
            "q": query,
            "sort": "stars",
            "order": "desc",
            "per_page": per_page,
            "page": page
        }
        url = f"{BASE_URL}/search/repositories"
        response = make_api_request(url, params)
        if response and "items" in response:
            all_repos.extend(response["items"])
        else:
            break  # No more pages
    return all_repos

"""#4.2 Data wrangling functions

"""

def clean_repos_data(repos_list):

    df = pd.DataFrame(repos_list)

    df['created_at'] = pd.to_datetime(df['created_at'])
    df['description'] = df['description'].fillna('No description')
    df['owner_name'] = df['owner'].apply(lambda x: x['login'])

    clean_df = df[['name', 'full_name', 'description', 'language',
                   'stargazers_count', 'created_at', 'owner_name']]

    print(f"Cleaned {len(clean_df)} repositories")
    return clean_df

def clean_commits_data(commits_list):

    df = pd.DataFrame(commits_list)

    df['author_name'] = df['commit'].apply(lambda x: x.get('author', {}).get('name', 'Unknown'))
    df['author_email'] = df['commit'].apply(lambda x: x.get('author', {}).get('email', 'Unknown'))
    df['commit_date'] = df['commit'].apply(lambda x: x.get('author', {}).get('date', ''))
    df['message'] = df['commit'].apply(lambda x: x.get('message', ''))

    df['commit_date'] = pd.to_datetime(df['commit_date'])

    df['message_clean'] = df['message'].str.strip().str.replace('\n', ' ')

    clean_df = df[['sha', 'author_name', 'author_email', 'commit_date',
                   'message', 'message_clean']]

    print(f"Cleaned {len(clean_df)} commits")
    return clean_df

"""#5.1 Examples basic functions"""

repo_1 = search_repositories("tetris language:assembly", per_page=5, sort = 'name', order="asc")

repo_1

comm_1 = get_repository_commits("kirjavascript", 'TetrisGYM', per_page=15)

comm_1

repo_content1 = get_repository_contents("kirjavascript", 'TetrisGYM', path="README.md")

repo_content1

"""#5.1 Examples data wrangling functions"""

clean_repos_data(repo_1)

clean_commits_data(comm_1)

"""#6.1. Test 1: Bad authentication"""

BASE_URL = "https://api.github.com"

TOKEN = "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"

HEADERS = {
    "Authorization": f"token {TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

response = requests.get("https://api.github.com/user", headers=HEADERS)
print(response.status_code, response.text)

BASE_URL = "https://api.github.com"

TOKEN = "github_123456789"

HEADERS = {
    "Authorization": f"token {TOKEN}",
    "Accept": "application/vnd.github.v3+json"
}

response = requests.get("https://api.github.com/user", headers=HEADERS)
print(response.status_code, response.text)

"""#6.2. Test 2: Rate limit checking"""

print("Remaining:", response.headers.get("X-RateLimit-Remaining"))

"""#6.3 Test 3: Pagination"""

repos = paginate_repositories("tetris language:assembly", max_pages=2)
print(f"Total repos fetched: {len(repos)}")
print("Example repo:", repos[0]["full_name"])

def paginate_repositories(query, max_pages=3, per_page=30):
    all_repos = []
    for page in range(1, max_pages + 1):
        params = {
            "q": query,
            "sort": "stars",
            "order": "desc",
            "per_page": per_page,
            "page": page
        }
        url = f"{BASE_URL}/search/repositories"
        response = make_api_request(url, params)
        if response and "items" in response:
            all_repos.extend(response["items"])
        else:
            break
    return all_repos

repos = paginate_repositories("tetris language:assembly", max_pages=2)
print(f"Total repos fetched: {len(repos)}")
print("Example repo:", repos[0]["full_name"])